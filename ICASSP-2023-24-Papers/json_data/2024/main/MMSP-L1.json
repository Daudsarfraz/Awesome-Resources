[
  {
    "title": "A Relation-Aware Heterogeneous Graph Transformer on Dynamic Fusion for Multimodal Classification Tasks",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446972",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Multi-Source Dynamic Interactive Network Collaborative Reasoning Image Captioning",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446104",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-Training and Multi-Modal Tokens",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446888",
    "github": "ms-dot-k/Image-to-Speech",
    "web_page": null,
    "github_page": "https://ms-dot-k.github.io/Image-to-Speech-Captioning/",
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": "2309.08531",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Textual Tokens Classification for Multi-Modal Alignment in Vision-Language Tracking",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446122",
    "github": "jankin987/ttctrack",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Caption Unification for Multi-View Lifelogging Images based on In-Context Learning with Heterogeneous Semantic Contents",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10445969",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "ControlCap: Controllable Captioning via No-Fuss Lexicon",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447133",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Large Language Models Augmented Rating Prediction in Recommender System",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447514",
    "github": "sichunluo/LAMAR",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Prompting Large Language Models with Fine-Grained Visual Relations from Scene Graph for Visual Question Answering",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10448321",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Learning Density Regulated and Multi-View Consistent Unsigned Distance Fields",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447163",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Graph-based Environment Representation for Vision-and-Language Navigation in Continuous Environments",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446850",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": "2301.04352",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Human Motion Capture Data Segmentation based on ST-GCN",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446553",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  },
  {
    "title": "Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10445931",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": "2309.09405",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Processing: Vision + Language"
  }
]