[
  {
    "title": "AttA-NET: Attention Aggregation Network for Audio-Visual Emotion Recognition",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447640",
    "github": "NariFan2002/AttA-NET",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Fusing Modality-Specific Representations and Decisions for Multimodal Emotion Recognition",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447035",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Speaker-Centric Multimodal Fusion Networks for Emotion Recognition in Conversations",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447720",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "CLIP-MSA: Incorporating Inter-Modal Dynamics and Common Knowledge to Multimodal Sentiment Analysis With Clip",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446825",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Circular Decomposition and Cross-Modal Recombination for Multimodal Sentiment Analysis",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446166",
    "github": "nianhua20/GCD-CMR",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "A Novel Multimodal Sentiment Analysis Model Based on Gated Fusion and Multi-Task Learning",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446040",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Modality-Dependent Sentiments Exploring for Multi-Modal Sentiment Classification",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10445820",
    "github": "royal-dargon/MDSE",
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Emotion-Aligned Contrastive Learning Between Images and Music",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447276",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": "2308.12610",
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "MMRBN: Rule-Based Network for Multimodal Emotion Recognition",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10447930",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "Inter-Modality and Intra-Sample Alignment for Multi-Modal Emotion Recognition",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446571",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  },
  {
    "title": "MDAVIF: A Multi-Domain Acoustical-Visual Information Fusion Model for Depression Recognition from Vlog Data",
    "base_url": null,
    "title_page": null,
    "ieee_id": "10446491",
    "github": null,
    "web_page": null,
    "github_page": null,
    "colab": null,
    "modelscope": null,
    "gitee": null,
    "gitlab": null,
    "zenodo": null,
    "kaggle": null,
    "demo_page": null,
    "paper_thecvf": null,
    "paper_arxiv_id": null,
    "paper_pdf": null,
    "paper_hal_science": null,
    "paper_researchgate": null,
    "paper_amazon": null,
    "youtube_id": null,
    "drive_google": null,
    "dropbox": null,
    "onedrive": null,
    "loom": null,
    "section": "Multimodal Emotion/Sentiment Analysis"
  }
]