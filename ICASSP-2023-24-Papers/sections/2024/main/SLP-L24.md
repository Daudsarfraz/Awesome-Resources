# ICASSP-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/ICASSP-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/SLP-P7.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/AASP-P7.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Resource Constrained Acoustic and Language Modeling

![Section Papers](https://img.shields.io/badge/Section%20Papers-24-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-17-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-2-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Automatic Data Augmentation for Disordered Speech Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447627-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447627) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2312.08641-b31b1b.svg)](https://arxiv.org/abs/2312.08641) | :heavy_minus_sign: |
| Soft Alignment of Modality Space for End-to-end Speech Translation | [![GitHub](https://img.shields.io/github/stars/MuKai2000/S-Align?style=flat)](https://github.com/MuKai2000/S-Align) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447494-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447494) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2312.10952-b31b1b.svg)](https://arxiv.org/abs/2312.10952) | :heavy_minus_sign: |
| Generative Context-Aware Fine-tuning of Self-Supervised Speech Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446893-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446893) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2312.09895-b31b1b.svg)](https://arxiv.org/abs/2312.09895) | :heavy_minus_sign: |
| Extending Large Language Models For Speech And Audio Captioning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446343-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446343) | :heavy_minus_sign: |
| Hystoc: Obtaining Word Confidences for Fusion of End-To-End ASR Systems | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446739-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446739) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2305.12579-b31b1b.svg)](https://arxiv.org/abs/2305.12579) | :heavy_minus_sign: |
| Effective Internal Language Model Training and Fusion for Factorized Transducer Model | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446240-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446240) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2404.01716-b31b1b.svg)](https://arxiv.org/abs/2404.01716) | :heavy_minus_sign: |
| Contextual Biasing of Named-Entities with Large Language Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10445918-E4A42C.svg)](https://ieeexplore.ieee.org/document/10445918) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.00723-b31b1b.svg)](https://arxiv.org/abs/2309.00723) | :heavy_minus_sign: |
| Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446299-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446299) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.16082-b31b1b.svg)](https://arxiv.org/abs/2309.16082) | :heavy_minus_sign: |
| Speaker-Adaptive Lipreading Via Spatio-Temporal Information Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447378-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447378) | :heavy_minus_sign: |
| A Multimodal Approach to Device-Directed Speech Detection with Large Language Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446224-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446224) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2403.14438-b31b1b.svg)](https://arxiv.org/abs/2403.14438) | :heavy_minus_sign: |
| Towards a World-English Language Model for On-Device Virtual Assistants | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448018-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448018) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2403.18783-b31b1b.svg)](https://arxiv.org/abs/2403.18783) | :heavy_minus_sign: |
| Correction Focused Language Model Training for Speech Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447802-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447802) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2310.11003-b31b1b.svg)](https://arxiv.org/abs/2310.11003) | :heavy_minus_sign: |
| Acoustic BPE for Speech Generation with Discrete Tokens | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446063-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446063) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2310.14580-b31b1b.svg)](https://arxiv.org/abs/2310.14580) | :heavy_minus_sign: |
| Study of Abuse Detection in Continuous Speech for Indian Languages | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447259-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447259) | :heavy_minus_sign: |
| Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447196-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447196) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2401.14717-b31b1b.svg)](https://arxiv.org/abs/2401.14717) | :heavy_minus_sign: |
| Accent-Specific Vector Quantization for Joint Unsupervised and Supervised Training in Accent Robust Speech Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446365-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446365) | :heavy_minus_sign: |
| TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448025-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448025) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.01947-b31b1b.svg)](https://arxiv.org/abs/2309.01947) | :heavy_minus_sign: |
| A Cross Search Method for Data Augmentation in Neural Machine Translation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447171-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447171) | :heavy_minus_sign: |
| ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for Transformer Layers | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446321-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446321) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2310.02489-b31b1b.svg)](https://arxiv.org/abs/2310.02489) | :heavy_minus_sign: |
| Robust Speaker Personalisation Using Generalized Low-Rank Adaptation for Automatic Speech Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446630-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446630) | :heavy_minus_sign: |
| Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446456-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446456) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.09920-b31b1b.svg)](https://arxiv.org/abs/2309.09920) | :heavy_minus_sign: |
| Improving Speed/Accuracy Tradeoff for Online Streaming ASR via Real-Valued and Trainable Strides | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446584-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446584) | :heavy_minus_sign: |
| Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447827-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447827) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.07988-b31b1b.svg)](https://arxiv.org/abs/2309.07988) | :heavy_minus_sign: |
| Enhancing Quantised End-to-End ASR Models via Personalisation | [![GitHub](https://img.shields.io/github/stars/qmgzhao/PQM?style=flat)](https://github.com/qmgzhao/PQM) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448012-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448012) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2309.09136-b31b1b.svg)](https://arxiv.org/abs/2309.09136) | :heavy_minus_sign: |