# ICASSP-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/ICASSP-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/AASP-P3.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/MLSP-P7.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Reinforcement Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-18-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-4-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multi-Agent Exploration via Self-Learning and Social Learning | [![GitHub](https://img.shields.io/github/stars/Shaokang-Agent/S2L?style=flat)](https://github.com/Shaokang-Agent/S2L) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446068-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446068) | :heavy_minus_sign: |
| M<sup>3</sup>ARL: Moment-Embedded Mean-Field Multi-Agent Reinforcement Learning for Continuous Action Space | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448058-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448058) | :heavy_minus_sign: |
| Zero-Shot Imitation Policy via Search in Demonstration Dataset | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447339-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447339) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.16398-b31b1b.svg)](https://arxiv.org/abs/2401.16398) | :heavy_minus_sign: |
| Adaptive Parameter Sharing for Multi-Agent Reinforcement Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447262-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447262) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2312.09009-b31b1b.svg)](https://arxiv.org/abs/2312.09009) | :heavy_minus_sign: |
| A Meta-Preconditioning Approach for Deep Q-Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446137-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446137) | :heavy_minus_sign: |
| MEPE: A Minimalist Ensemble Policy Evaluation Operator for Deep Reinforcement Learning | [![GitHub](https://img.shields.io/github/stars/sweetice/MEPE?style=flat)](https://github.com/sweetice/MEPE) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448009-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448009) | :heavy_minus_sign: |
| Tensor Low-Rank Approximation of Finite-Horizon Value Functions | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448465-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448465) | :heavy_minus_sign: |
| Offline Reinforcement Learning with Generative Adversarial Networks and Uncertainty Estimation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446266-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446266) | :heavy_minus_sign: |
| Offline Reinforcement Learning based on Next State Supervision | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446781-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446781) | :heavy_minus_sign: |
| Proximal Bellman Mappings for Reinforcement Learning and Their Application to Robust Adaptive Filtering | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446701-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446701) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07548-b31b1b.svg)](https://arxiv.org/abs/2309.07548) | :heavy_minus_sign: |
| A Robust Quantile Huber Loss with Interpretable Parameter Adjustment in Distributional Reinforcement Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447501-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447501) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2401.02325-b31b1b.svg)](https://arxiv.org/abs/2401.02325) | :heavy_minus_sign: |
| Graph-Enhanced Hybrid Sampling for Multi-Armed Bandit Recommendation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446562-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446562) | :heavy_minus_sign: |
| Interpretable Policy Extraction with Neuro-Symbolic Reinforcement Learning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446037-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446037) | :heavy_minus_sign: |
| Self-Supervised Reinforcement Learning for Out-of-Distribution Recovery via Auxiliary Reward | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447216-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447216) | :heavy_minus_sign: |
| Offline Reinforcement Learning with Policy Guidance and Uncertainty Estimation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447908-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447908) | :heavy_minus_sign: |
| Multi-Agent Sparse Interaction Modeling is an Anomaly Detection Problem | [![GitHub](https://img.shields.io/github/stars/chaobiubiu/Sparse-Interaction-as-Anomaly?style=flat)](https://github.com/chaobiubiu/Sparse-Interaction-as-Anomaly) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446644-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446644) | :heavy_minus_sign: |
| A New Pre-Training Paradigm for Offline Multi-Agent Reinforcement Learning with Suboptimal Data | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448500-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448500) | :heavy_minus_sign: |
| Trend-Heuristic Reinforcement Learning Framework for News-Oriented Stock Portfolio Management | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447993-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447993) | :heavy_minus_sign: |
