# ICASSP-2024-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
    <tr>
        <td><strong>Previous Collections</strong></td>
        <td>
            <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/README_2023.md">
                <img src="http://img.shields.io/badge/ICASSP-2023-0073AE.svg" alt="Conference">
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/SAM-P1.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICASSP-2023-24-Papers/blob/main/sections/2024/main/AASP-P9.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>


## Multimodal Processing of Language

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-3-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-1-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Cooking-Clip: Context-Aware Language-Image Pretraining for Zero-Shot Recipe Generation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447148-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447148) | :heavy_minus_sign: |
| Exploring Object-Centered External Knowledge for Fine-Grained Video Paragraph Captioning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448104-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448104) | :heavy_minus_sign: |
| Relational Graph-Bridged Image-Text Interaction: A Novel Method for Multi-Modal Relation Extraction | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448507-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448507) | :heavy_minus_sign: |
| DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448111-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448111) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2401.01076-b31b1b.svg)](https://arxiv.org/abs/2401.01076) | :heavy_minus_sign: |
| Vector Quantization Knowledge Transfer for End-to-End Text Image Machine Translation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447334-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447334) | :heavy_minus_sign: |
| EmoRED: A Dataset for Relation Extraction in Texts with Emoticons | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448038-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448038) | :heavy_minus_sign: |
| MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder Language Model for Video-grounded Dialogue Generation | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447469-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447469) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2311.12820-b31b1b.svg)](https://arxiv.org/abs/2311.12820) | :heavy_minus_sign: |
| CausalME: Balancing bi-modalities in Visual Question Answering | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447342-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447342) | :heavy_minus_sign: |
| MHPS: Multimodality-Guided Hierarchical Policy Search for Knowledge Graph Reasoning | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447436-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447436) | :heavy_minus_sign: |
| Empowering Vision-Language Models for Reasoning Ability through Large Language Models | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446407-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446407) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2305.13267-b31b1b.svg)](https://arxiv.org/abs/2305.13267) | :heavy_minus_sign: |
| PVCG: Prompt-Based Vision-Aware Classification and Generation for Multi-Modal Rumor Detection | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10447285-E4A42C.svg)](https://ieeexplore.ieee.org/document/10447285) | :heavy_minus_sign: |
| LabCLIP: Label-Enhanced Clip for Improving Zero-Shot Text Classification | [![GitHub](https://img.shields.io/github/stars/BRZ911/LABCLIP?style=flat)](https://github.com/BRZ911/LABCLIP) | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10446865-E4A42C.svg)](https://ieeexplore.ieee.org/document/10446865) | :heavy_minus_sign: |
| Context-Aware Dual Attention Network for Multimodal Sarcasm Detection | :heavy_minus_sign: | [![IEEE Xplore](https://img.shields.io/badge/IEEE-10448377-E4A42C.svg)](https://ieeexplore.ieee.org/document/10448377) | :heavy_minus_sign: |

