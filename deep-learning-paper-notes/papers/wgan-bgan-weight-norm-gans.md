* Jan 26, 2017, 197 citations: [Wasserstein GAN](https://github.com/hoangcuong2011/Good-Papers/blob/master/Wasserstein%20GAN.md): the goal of a GAN is to create a distribution that maximizes the likelihood of observing the training data set. Traditionally GANs would measure the divergence between the training data set and the set generated by the generator net with JS or KL divergence, but these weren't a good match - "JS distance wasn't defined for all gradients". Wasserstein GAN proposed using Earth Mover (AKA Wasserstein) distance as the loss function, which has the nice properties of being continuous and differentiable everywhere. However, Wasserstein distance is intractable to compute, so WGANs approximate it with another tiny neural net, and as a result GANs a) converged more stably, b) don't mode collapse (generator always produces same image), c) there's a relationship between loss and image quality, which didn't exist in JS-divergence-based-GANs. Everyone cheers! Widely loved paper.

* Mar 31, 2017, 43 citations: [BEGAN: Boundary Equilibrium Generative Adversarial Networks](https://github.com/aleju/papers/blob/master/neural-nets/BEGAN.md): the discriminator is an autoencoder, and the autoencoder has its own loss function (separate from the GAN's loss function) that makes it minimize the reconstruction loss from real images and maximize the reconstruction loss from fake images. The GAN loss function is the distance between the reconstruction losses. Too big, and the generator has to catch up by generating an image that has less reconstruction losses. Why was this SOTA as of April 2017?

* Apr 13, 2017, ? citations: [On the Effects of Batch and Weight Normalization in Generative Adversarial Networks](https://arxiv.org/abs/1704.03971) Batch norm can help GAN training quality in the beginning, but can negatively impact quality of trained model (evaluating this is tough though, as it's hard to evaluate the quality of a GAN, so they invent a metric). WGANs make GANs easier to train because their loss function is Lipschitz-1 continuous, so the loss function landscape is smoother; weight normalization are somehow similar, meaning this paper provides similarly easier training and reduced mode collapse like WGANs.
